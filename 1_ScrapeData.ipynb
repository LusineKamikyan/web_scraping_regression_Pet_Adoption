{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code that I used to scrape the petfinders.my website for pets that were put for adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_adopted_urls(url,runtime_regex):\n",
    "    '''(url string ---> list of adopted pets urls)\n",
    "    \n",
    "    This function takes a url of a page and returns the links to\n",
    "    the adopted peets on that page\n",
    "    \n",
    "    '''\n",
    "    adopted_urls_per_page = []\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    print(response.status_code)\n",
    "\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    maincontent = soup.find('div', id = 'maincontent')\n",
    "    list_adopted = maincontent.find_all('font', string = runtime_regex)\n",
    "    list_adopted.pop(0) # the first row contains the 'Adopted' but not a pet\n",
    "    for ix, entry in enumerate(list_adopted):\n",
    "        url = list_adopted[ix].parent.parent.parent.find('a').get(\"href\")\n",
    "        adopted_urls_per_page.append(url)\n",
    "    return adopted_urls_per_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_scraped_urls(num_pages):\n",
    "    ''' (integer ---> list) \n",
    "    \n",
    "    This function takes in the number of pages to be scraped\n",
    "    and returns the list of the urls\n",
    "    '''\n",
    "    url_start = 'https://www.petfinder.my/listings.php?page='\n",
    "    url_end = '&sorttype=&sortage=&sortadopt=1&sortfield=&sortbreed1=&sortbreed2=&sortstate=&sortquery='\n",
    "    \n",
    "    runtime_regex = re.compile('Adopted')\n",
    "    urls_adopted_list = []\n",
    "    for i in range(1,num_pages+1): \n",
    "        url = url_start+str(i)+url_end\n",
    "        adopted_urls_per_page = scrape_adopted_urls(url,runtime_regex)\n",
    "        #print(adopted_urls_per_page)\n",
    "        #urls_adopted_list.append(adopted_urls_per_page)\n",
    "        urls_adopted_list = urls_adopted_list + adopted_urls_per_page\n",
    "        time.sleep(30+1.5*random.random())\n",
    "\n",
    "    return urls_adopted_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pet_page(pet_url):\n",
    "    '''(string ---> dictionary)\n",
    "    \n",
    "    This function takes the url to a pet information page\n",
    "    and scrapes all the information into a dictionary\n",
    "    \n",
    "    '''\n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    \n",
    "    response = requests.get(pet_url, headers = user_agent)\n",
    "    print(response.status_code)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "    maincontent = soup.find('div', id = 'maincontent')\n",
    "\n",
    "    pet = maincontent.find('td', string = re.compile('Dog|Cat'))\n",
    "    \n",
    "    if pet != None:\n",
    "        pet_profile = pet.parent.parent\n",
    "\n",
    "        pet_profile_dictionary = {}\n",
    "        for i in pet_profile.find_all('tr'):\n",
    "            line = i.get_text(separator='|', strip=True).split('|')\n",
    "            if len(line)>1:\n",
    "                pet_profile_dictionary[line[0]] = line[1:]\n",
    "        pet_discription = maincontent.find('div', id = 'pet_desc').text\n",
    "        pet_profile_dictionary['Pet Discription'] = pet_discription    \n",
    "        return pet_profile_dictionary\n",
    "    else:\n",
    "        return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_pets(pet_urls):\n",
    "    '''This function takes a list of urls that contain pet profiles \n",
    "    and returns a list of dictionaries where each dictionary\n",
    "    is a pet profile\n",
    "    '''\n",
    "    pet_profiles = []\n",
    "    for url in pet_urls:\n",
    "        pet_profile_dict = scrape_pet_page(url)\n",
    "        if pet_profile_dict != {}:\n",
    "            pet_profiles.append(pet_profile_dict)\n",
    "            time.sleep(30+1.5*random.random())\n",
    "\n",
    "    return pet_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/urllib/request.py\", line 1317, in do_open\n",
      "    encode_chunked=req.has_header('Transfer-encoding'))\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/http/client.py\", line 1244, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/http/client.py\", line 1290, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/http/client.py\", line 1239, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/http/client.py\", line 1026, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/http/client.py\", line 966, in send\n",
      "    self.connect()\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/http/client.py\", line 938, in connect\n",
      "    (self.host,self.port), self.timeout, self.source_address)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/socket.py\", line 727, in create_connection\n",
      "    raise err\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/socket.py\", line 716, in create_connection\n",
      "    sock.connect(sa)\n",
      "socket.timeout: timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/site-packages/fake_useragent/utils.py\", line 67, in get\n",
      "    context=context,\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/urllib/request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/urllib/request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/urllib/request.py\", line 543, in _open\n",
      "    '_open', req)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/urllib/request.py\", line 503, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/urllib/request.py\", line 1345, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/urllib/request.py\", line 1319, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error timed out>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/site-packages/fake_useragent/utils.py\", line 166, in load\n",
      "    verify_ssl=verify_ssl,\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/site-packages/fake_useragent/utils.py\", line 122, in get_browser_versions\n",
      "    verify_ssl=verify_ssl,\n",
      "  File \"/opt/anaconda3/envs/metis/lib/python3.7/site-packages/fake_useragent/utils.py\", line 84, in get\n",
      "    raise FakeUserAgentError('Maximum amount of retries reached')\n",
      "fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# getting urls to pet profiles and saving them to a file\n",
    "pet_urls = append_scraped_urls(2)\n",
    "\n",
    "with open('pet_urls.txt', 'w') as f:\n",
    "    for item in pet_urls:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# getting the pet profiles and saving them into a file\n",
    "All_pets = accumulate_pets(pet_urls)\n",
    "with open('data.txt', 'w') as f:\n",
    "    for item in All_pets:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
